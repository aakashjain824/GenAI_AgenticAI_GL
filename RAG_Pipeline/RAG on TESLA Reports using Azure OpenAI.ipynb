{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-SSaZ34Ejlu"
   },
   "source": [
    "# Learning Objectives\n",
    "\n",
    "- Build an LLM assistant for document-based Q&A using retrieval-augmented generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UgAFFFruPmpH"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zuVrrCykDkmM"
   },
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BsxRRx5o7ccI"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "azure_api_key = os.getenv('azure_api_key')\n",
    "# Modify the Azure Endpoint and the API Versions as needed\n",
    "azure_base_url = os.getenv('azure_base_url')\n",
    "azure_api_version = os.getenv('azure_api_version')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "YxEMDfdxnzfJ"
   },
   "outputs": [],
   "source": [
    "client = AzureOpenAI(\n",
    "  azure_endpoint = azure_base_url,\n",
    "  api_key = azure_api_key,\n",
    "  api_version = azure_api_version\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "bqPNG9DFoKeU"
   },
   "outputs": [],
   "source": [
    "model_name = 'gpt-4o-mini' # deployment name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "GVwgNoHguTMN"
   },
   "outputs": [],
   "source": [
    "embedding_model = AzureOpenAIEmbeddings(\n",
    "    api_key = azure_api_key,\n",
    "    azure_endpoint = azure_base_url,\n",
    "    api_version = azure_api_version,\n",
    "    azure_deployment=\"text-embedding-3-large\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lqic3jajEMZZ"
   },
   "source": [
    "# Load the Vector Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7B_do80GZXDJ"
   },
   "source": [
    "Since we persisted the database to to a folder, we can use it from the given location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lCGSIZifZ8jF"
   },
   "source": [
    "In practise, the database is maintained as a separate entity and CRUD operations are managed just as one would for normal databases (e.g., relational databases)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Y_-X6UYZxr-"
   },
   "source": [
    "Now that the database is uploaded onto the Colab instance, we can unzip it and attach a retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 268,
     "status": "ok",
     "timestamp": 1753432751914,
     "user": {
      "displayName": "Henry Isaac",
      "userId": "09991029790815276174"
     },
     "user_tz": -330
    },
    "id": "_LuDOQpxumYO",
    "outputId": "8b6e691c-325b-4205-85b6-f42a8700c7dc"
   },
   "outputs": [],
   "source": [
    "chromadb_client = chromadb.PersistentClient(\n",
    "    path=\"./tesla_db\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "E17vG7WJvoaJ"
   },
   "outputs": [],
   "source": [
    "tesla_10k_collection = 'tesla-10k-2019-to-2023'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1753432751964,
     "user": {
      "displayName": "Henry Isaac",
      "userId": "09991029790815276174"
     },
     "user_tz": -330
    },
    "id": "o3VQmzZnuLzw",
    "outputId": "577f6da7-23b9-429b-a719-6836e975c013"
   },
   "outputs": [],
   "source": [
    "vectorstore_persisted = Chroma(\n",
    "    collection_name=tesla_10k_collection,\n",
    "    collection_metadata={\"hnsw:space\": \"cosine\"},\n",
    "    embedding_function=embedding_model,\n",
    "    client=chromadb_client,\n",
    "    persist_directory=\"./tesla_db\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "C-15bwukuVYU"
   },
   "outputs": [],
   "source": [
    "retriever = vectorstore_persisted.as_retriever(\n",
    "    search_type='similarity',\n",
    "    search_kwargs={'k': 5}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pv9oH4ukXLSa"
   },
   "source": [
    "# RAG Q&A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s343-PgW6P-K"
   },
   "source": [
    "## Prompt Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DFSzXhkA4Mmu"
   },
   "source": [
    "The RAG system message should clearly communicate to the LLM that the input will include a user query along with the necessary context information to address that query. Additionally, the response should rely solely on the context information provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "LR4dzgL96U0-"
   },
   "outputs": [],
   "source": [
    "qna_system_message = \"\"\"\n",
    "You are an assistant to a financial services firm who answers user queries on annual reports.\n",
    "User input will have the context required by you to answer user queries.\n",
    "This context will be delimited by: <Context> and </Context>.\n",
    "The context contains references to specific portions of a document relevant to the user query.\n",
    "\n",
    "User queries will be delimited by: <Question> and </Question>.\n",
    "\n",
    "Please answer user queries only using the context provided in the input.\n",
    "Do not mention anything about the context in your final answer. Your response should only contain the answer to the question.\n",
    "\n",
    "If the answer is not found in the context, respond \"I don't know\".\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "bDexqi8c6Xmm"
   },
   "outputs": [],
   "source": [
    "qna_user_message_template = \"\"\"\n",
    "<Context>\n",
    "Here are some documents that are relevant to the question mentioned below.\n",
    "{context}\n",
    "</Context>\n",
    "\n",
    "<Question>\n",
    "{question}\n",
    "</Question>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iT4QahJv6xTz"
   },
   "source": [
    "## Retrieving relevant documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "nsZuE-Xo2dAR"
   },
   "outputs": [],
   "source": [
    "user_query = \"What was the annual revenue of the company in 2022?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1480,
     "status": "ok",
     "timestamp": 1753432753568,
     "user": {
      "displayName": "Henry Isaac",
      "userId": "09991029790815276174"
     },
     "user_tz": -330
    },
    "id": "MUBRJsi12e59",
    "outputId": "566cb321-e161-4f1d-cd0d-4b8954b41b6e"
   },
   "outputs": [],
   "source": [
    "relevant_document_chunks = retriever.invoke(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 50,
     "status": "ok",
     "timestamp": 1753432753621,
     "user": {
      "displayName": "Henry Isaac",
      "userId": "09991029790815276174"
     },
     "user_tz": -330
    },
    "id": "7eH_q5P92gxJ",
    "outputId": "d1cbfe71-8d53-4129-f828-4370ec772ce4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(relevant_document_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcKi88RowP8W"
   },
   "source": [
    "We can inspect the first document like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1753432753644,
     "user": {
      "displayName": "Henry Isaac",
      "userId": "09991029790815276174"
     },
     "user_tz": -330
    },
    "id": "1KeoZOE62jF5",
    "outputId": "23d21c8f-0d0e-4e54-dc09-9a111163c6af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Furthermore, significant judgment is required in evaluating our tax positions. In the ordinary course of business, there are many transactions and\n",
      "calculations for which the ultimate tax settlement is uncertain. As a result, we recognize the effect of this uncertainty on our tax attributes or taxes\n",
      "payable based on our estimates of the eventual outcome. These effects are recognized when, despite our belief that our tax return positions are\n",
      "supportable, we believe that it is more likely than not that some of those positions may not be fully sustained upon review by tax authorities. We are\n",
      "required to file income tax returns in the U.S. and various foreign jurisdictions, which requires us to interpret the applicable tax laws and regulations in\n",
      "effect in such jurisdictions. Such returns are subject to audit by the various federal, state and foreign taxing authorities, who may disagree with respect to\n",
      "our tax positions. We believe that our consideration is adequate for all open audit years based on our assessment of many factors, including past\n",
      "experience and interpretations of tax law. We review and update our estimates in light of changing facts and circumstances, such as the closing of a tax\n",
      "audit, the lapse of a statute of limitations or a change in estimate. To the extent that the final tax outcome of these matters differs from our expectations,\n",
      "such differences may impact income tax expense in the period in which such determination is made.\n",
      "Results of Operations\n",
      "Revenues\n",
      " \n",
      "Year Ended December 31,\n",
      "2023 vs. 2022 Change\n",
      "2022 vs. 2021 Change\n",
      "(Dollars in millions)\n",
      "2023\n",
      "2022\n",
      "2021\n",
      "$\n",
      "%\n",
      "$\n",
      "%\n"
     ]
    }
   ],
   "source": [
    "for document in relevant_document_chunks:\n",
    "    print(document.page_content.replace(\"\\t\", \" \"))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G_ekBjVM60P1"
   },
   "source": [
    "## Composing the response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mO6P2A3J4pkx"
   },
   "source": [
    "To compose the response to user queries, we assemble the prompt that uses the system message defined above and the dynamically retrieved context for the user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "4WZhSBsI7Av7"
   },
   "outputs": [],
   "source": [
    "user_query = \"What was the annual revenue of the company in 2022?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 945,
     "status": "ok",
     "timestamp": 1753432754598,
     "user": {
      "displayName": "Henry Isaac",
      "userId": "09991029790815276174"
     },
     "user_tz": -330
    },
    "id": "aHXY6BcV676h",
    "outputId": "79e29419-6505-44dc-e744-6354ff03eee7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The annual revenue of the company in 2022 was $96.77 billion.\n"
     ]
    }
   ],
   "source": [
    "relevant_document_chunks = retriever.invoke(user_query)\n",
    "context_list = [d.page_content for d in relevant_document_chunks]\n",
    "context_for_query = \"\\n---\\n\".join(context_list)\n",
    "\n",
    "prompt = [\n",
    "    {'role': 'developer', 'content': qna_system_message},\n",
    "    {'role': 'user', 'content': qna_user_message_template.format(\n",
    "         context=context_for_query,\n",
    "         question=user_query\n",
    "        )\n",
    "    }\n",
    "]\n",
    "\n",
    "try:\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=prompt,\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    prediction = response.choices[0].message.content.strip()\n",
    "except Exception as e:\n",
    "    prediction = f'Sorry, I encountered the following error: \\n {e}'\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9DuBmLD4wgLP"
   },
   "source": [
    "# A RAG Assistant using Gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y1B5jZsbxAFn"
   },
   "source": [
    "Let us put together the code in this notebook into a file `rag-chat.py` that will open up a basic command line chat interface whenever it is run at the terminal. This naive implementation neverthless illustrates how document Q&A could be automated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ro6WVZrI3mxR"
   },
   "source": [
    "Test Queries:\n",
    "- What was the total revenue of the company in 2022?\n",
    "- Summarize 5 key risks identified in the 2023 10k report? Respond with bullet point summaries.\n",
    "- What is the view of the management on the future of electric vehicle batteries?\n",
    "- What was the company's debt level in 2023?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zenbook\\AppData\\Local\\Temp\\ipykernel_26792\\2429889782.py:126: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(label=\"📊 Assistant\", show_copy_button=True, bubble_full_width=False)\n",
      "C:\\Users\\Zenbook\\AppData\\Local\\Temp\\ipykernel_26792\\2429889782.py:126: DeprecationWarning: The 'bubble_full_width' parameter is deprecated and will be removed in a future version. This parameter no longer has any effect.\n",
      "  chatbot = gr.Chatbot(label=\"📊 Assistant\", show_copy_button=True, bubble_full_width=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "* Running on public URL: https://a4c2ab2527a9bddfe3.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://a4c2ab2527a9bddfe3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import chromadb\n",
    "import gradio as gr\n",
    "\n",
    "from openai import AzureOpenAI\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Configuration\n",
    "model_name = 'gpt-4o-mini'\n",
    "tesla_10k_collection = 'tesla-10k-2019-to-2023'\n",
    "\n",
    "azure_api_key = os.getenv('azure_api_key')\n",
    "azure_base_url = os.getenv('azure_base_url')\n",
    "azure_api_version = os.getenv('azure_api_version')\n",
    "\n",
    "# Azure OpenAI Client\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=azure_base_url,\n",
    "    api_key=azure_api_key,\n",
    "    api_version=azure_api_version\n",
    ")\n",
    "\n",
    "# Embedding Model\n",
    "embedding_model = AzureOpenAIEmbeddings(\n",
    "    api_key=azure_api_key,\n",
    "    azure_endpoint=azure_base_url,\n",
    "    api_version=azure_api_version,\n",
    "    azure_deployment=\"text-embedding-3-large\"\n",
    ")\n",
    "\n",
    "# Vector Store (Chroma)\n",
    "chromadb_client = chromadb.PersistentClient(path=\"./tesla_db\")\n",
    "\n",
    "vectorstore_persisted = Chroma(\n",
    "    collection_name=tesla_10k_collection,\n",
    "    collection_metadata={\"hnsw:space\": \"cosine\"},\n",
    "    embedding_function=embedding_model,\n",
    "    client=chromadb_client,\n",
    "    persist_directory=\"./tesla_db\"\n",
    ")\n",
    "\n",
    "retriever = vectorstore_persisted.as_retriever(\n",
    "    search_type='similarity',\n",
    "    search_kwargs={'k': 5}\n",
    ")\n",
    "\n",
    "# Prompt Template\n",
    "qna_system_message = \"\"\"\n",
    "You are an assistant to a financial services firm who answers user queries on annual reports.\n",
    "User input will have the context required by you to answer user queries.\n",
    "This context will be delimited by: <Context> and </Context>.\n",
    "The context contains references to specific portions of a document relevant to the user query.\n",
    "\n",
    "User queries will be delimited by: <Question> and </Question>.\n",
    "\n",
    "Please answer user queries only using the context provided in the input.\n",
    "Do not mention anything about the context in your final answer. Your response should only contain the answer to the question.\n",
    "\n",
    "If the answer is not found in the context, respond \"I don't know\".\n",
    "\"\"\"\n",
    "\n",
    "qna_user_message_template = \"\"\"\n",
    "<Context>\n",
    "Here are some documents that are relevant to the question mentioned below.\n",
    "{context}\n",
    "</Context>\n",
    "\n",
    "<Question>\n",
    "{question}\n",
    "</Question>\n",
    "\"\"\"\n",
    "\n",
    "# Core response logic\n",
    "def respond(user_query):\n",
    "    relevant_document_chunks = retriever.invoke(user_query)\n",
    "    context_list = [d.page_content for d in relevant_document_chunks]\n",
    "    context_for_query = \"\\n---\\n\".join(context_list)\n",
    "\n",
    "    prompt = [\n",
    "        {'role': 'developer', 'content': qna_system_message},\n",
    "        {\n",
    "            'role': 'user', 'content': qna_user_message_template.format(\n",
    "                context=context_for_query,\n",
    "                question=user_query)\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=prompt,\n",
    "            temperature=0\n",
    "        )\n",
    "\n",
    "        answer = response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        answer = f'Sorry, I encountered the following error:\\n{e}'\n",
    "\n",
    "    return answer\n",
    "\n",
    "# Gradio handler\n",
    "def chat_interface(user_input, chat_history):\n",
    "    try:\n",
    "        response = respond(user_input)\n",
    "    except Exception as e:\n",
    "        response = f\"❌ Error: {str(e)}\"\n",
    "    chat_history.append((user_input, response))\n",
    "    return \"\", chat_history\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "def clear_chat():\n",
    "    return [], \"\"\n",
    "\n",
    "with gr.Blocks(title=\"RAG Chat - Tesla 10K Assistant\", theme=gr.themes.Soft()) as demo:\n",
    "    # Header\n",
    "    gr.Markdown(\"\"\"\n",
    "    <h1 style=\"text-align: center;\">🤖 Tesla 10-K Financial Assistant</h1>\n",
    "    <p style=\"text-align: center;\">Ask questions about Tesla's annual reports (2019–2023)</p>\n",
    "    \"\"\", elem_id=\"header\")\n",
    "\n",
    "    # Chatbot + Input Area\n",
    "    with gr.Column(variant=\"panel\"):\n",
    "        chatbot = gr.Chatbot(label=\"📊 Assistant\", show_copy_button=True, bubble_full_width=False)\n",
    "        with gr.Row():\n",
    "            msg = gr.Textbox(\n",
    "                placeholder=\"Type your financial question here...\",\n",
    "                label=\"Your Question\",\n",
    "                scale=10\n",
    "            )\n",
    "            send_btn = gr.Button(\"🚀 Send\", scale=2)\n",
    "\n",
    "    # Footer Buttons\n",
    "    with gr.Row():\n",
    "        clear_btn = gr.Button(\"🧹 Clear Chat\", variant=\"stop\")\n",
    "\n",
    "    # Function bindings\n",
    "    send_btn.click(chat_interface, [msg, chatbot], [msg, chatbot])\n",
    "    msg.submit(chat_interface, [msg, chatbot], [msg, chatbot])\n",
    "    clear_btn.click(clear_chat, [], [chatbot, msg])\n",
    "\n",
    "# Launch app\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3qQfjF5XAsU3"
   },
   "source": [
    "<font size=6; color='blue'> **Happy Learning!** </font>\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ZwfVXvnAsiu"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "genai-agenticai-gl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
